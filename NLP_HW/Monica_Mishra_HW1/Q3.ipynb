{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 \n",
    "\n",
    "Language Modeling (30 points)\n",
    "\n",
    "Implement a 4-gram language model from the following English training data: http://www.nltk.org/nltk_data/packages/corpora/gutenberg.zip\n",
    "Across all files in the directory (counted together), report the 4-gram counts.\n",
    "Refer instructions below.\n",
    "\n",
    "\n",
    "\n",
    "For the given datasets:\n",
    "News Articles:\n",
    "https://drive.google.com/open?id=14veC-71fA_pE-kR8GUz8xwV37NehPrA2\n",
    "\n",
    "IMDB Movie Reviews:\n",
    "https://drive.google.com/open?id=1AaErcFL4H30LVdiObQoY4w7MIVzPCVXq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get data and preprocess it\n",
    "def data_extractor(dataset):\n",
    "    complete_data = []\n",
    "    filename = os.getcwd() + \"\\\\\" + dataset\n",
    "    for text in os.listdir(filename):\n",
    "        filename = filename + \"\\\\\" + text\n",
    "        if \".DS_Store\" not in filename:\n",
    "            with open(filename) as file:\n",
    "                text_in_file = file.read()\n",
    "                text_in_file = text_in_file.lower() ## make all text lowercase\n",
    "                text_in_file = text_in_file.replace(\"[\", \"[ \").replace(\"]\", \" ]\").replace(\")\", \") \").replace(\"(\", \" (\").replace(\"}\", \" }\").replace(\"{\", \"{ \")\n",
    "                text_in_file = re.sub(\"\\n+\", \" \", text_in_file) ## remove extra lines\n",
    "                text_in_file = re.sub(\"\\s+\", \" \", text_in_file) ## remove extra spaces\n",
    "            text_in_file += \" \"\n",
    "            complete_data.append(text_in_file)\n",
    "        filename = os.getcwd() + \"\\\\\" + dataset\n",
    "    return complete_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "Calculate the average perplexity for each dataset. Use lambda smoothing (with lambda =\n",
    "0.1)\n",
    "\n",
    "\n",
    "For preprocessing the data, see instructions below.\n",
    "Data preprocessing instructions for 3.1 and 3.2:\n",
    "1. Remove blank lines from each file.\n",
    "2. Replace newline characters with spaces.\n",
    "3. Remove duplicate spaces.\n",
    "4. Tokenize each document (you can use NLTK, https://www.nltk.org/).\n",
    "5. Replace words in train set that appear less than 5 times as \\UNK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get data in as a single string then split to get each token, calculate unigram count\n",
    "text_in_file = data_extractor(dataset = \"gutenberg\")\n",
    "complete_data = \" \".join(text_in_file)\n",
    "complete_data_list = complete_data.split(\" \")\n",
    "len_set_complete_data = len(list(set(complete_data_list)))\n",
    "data_unigram_count = Counter(complete_data_list)\n",
    "data_count_less_than_5 = {i: data_unigram_count[i] for i in data_unigram_count if data_unigram_count[i] < 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the less frequent words to \"UNK\" token\n",
    "for index, i in enumerate(data_count_less_than_5):\n",
    "    word = \" \" + i + \" \"\n",
    "    complete_data = complete_data.replace(word , \" UNK \")\n",
    "    if index % 1000 == 0:\n",
    "        print (index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get tri grams and their count\n",
    "complete_data_list = complete_data.split(\" \")\n",
    "all_3_grams = [\" \".join(complete_data_list[index:index + 3]) for index in range(len(complete_data_list[:-2]))]\n",
    "count_3_grams = Counter(all_3_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get four grams and their count\n",
    "all_4_grams = [\" \".join(complete_data_list[index:index + 4]) for index in range(len(complete_data_list[:-3]))]\n",
    "count_4_grams = Counter(all_4_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "For the given datasets:\n",
    "News Articles:\n",
    "\n",
    "https://drive.google.com/open?id=14veC-71fA_pE-kR8GUz8xwV37NehPrA2\n",
    "IMDB Movie Reviews:\n",
    "\n",
    "https://drive.google.com/open?id=1AaErcFL4H30LVdiObQoY4w7MIVzPCVXq\n",
    "\n",
    "Calculate the average perplexity for each dataset. Use lambda smoothing (with lambda =\n",
    "0.1)\n",
    "For preprocessing the data, see instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## find perplexity based on training count\n",
    "def calculate_perplexity(test_complete_dataset):\n",
    "    all_perplexity = []\n",
    "    for test_set in test_complete_dataset[:1]:\n",
    "        test_set_list = test_set.split(\" \")\n",
    "        not_preset_in_training_words = set(test_set_list) - set(complete_data_list) \n",
    "        for i in not_preset_in_training_words:\n",
    "            word = \" \" + i + \" \"\n",
    "            test_set = test_set.replace(word , \" UNK \")  \n",
    "        test_set_list = test_set.split(\" \")\n",
    "        test_4_grams = [\" \".join(test_set_list[index:index + 4]) for index in range(len(test_set_list[:-3]))]\n",
    "        lambda_val = 0.1\n",
    "        test_prob_4_gram = {}\n",
    "        vocab = len(set(test_set_list))\n",
    "        for four_gram in test_4_grams:\n",
    "            word_in_three_gram = i.split(\" \")[:-1]\n",
    "            three_gram = \" \".join(word_in_three_gram)\n",
    "            if four_gram not in count_4_grams:\n",
    "                count_4_grams[four_gram] = 0\n",
    "            if three_gram not in count_3_grams:\n",
    "                count_3_grams[three_gram] = 0\n",
    "            test_prob_4_gram[four_gram] = (count_4_grams[four_gram] + lambda_val)/ (count_3_grams[three_gram] + lambda_val * vocab)\n",
    "        entropy = 0\n",
    "        for i in test_prob_4_gram:\n",
    "            entropy += math.log(test_prob_4_gram[i])\n",
    "        perplexity_per_doc = (math.e ** (-1/(len(test_set_list)) * entropy))\n",
    "        all_perplexity.append(perplexity_per_doc)\n",
    "    avg_perplexity = sum(all_perplexity)/len(all_perplexity)\n",
    "    return avg_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## call dataset gatherer and perplexity calculator\n",
    "def get_avg_perlexity(dataset):\n",
    "    test_complete_dataset = data_extractor(dataset)\n",
    "    avg_perplexity = calculate_perplexity(test_complete_dataset)\n",
    "    print (\"Average perplexity for {0} dataset: {1}\".format(dataset, avg_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity for imdb_data dataset: 70.17773402921019\n"
     ]
    }
   ],
   "source": [
    "get_avg_perlexity(dataset = \"imdb_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity for news_data dataset: 128.4372991933948\n"
     ]
    }
   ],
   "source": [
    "get_avg_perlexity(dataset = \"news_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
