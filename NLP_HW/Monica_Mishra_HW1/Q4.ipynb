{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "POS Tagging - HMM (50 points)\n",
    "The training dataset is a subset of the Brown corpus, where each file contains sentences of\n",
    "tokenized words followed by POS tags, and where each line contains one sentence:\n",
    "https://www.dropbox.com/sh/havbkrjqzu9kpv6/AABVY0xRUvu-AO2TyftQUwCEa?dl=0\n",
    "The test dataset (which is another subset of the Brown corpus, containing tokenized words\n",
    "but no tags) is the following:\n",
    "\n",
    "\n",
    "https://www.dropbox.com/s/5j62js3pa0z6z9e/science_sample.txt?dl=0\n",
    "Information regarding the categories of the dataset can be found at:\n",
    "https://www.dropbox.com/s/ujnz04e62e9603j/CONTENTS.txt?dl=0\n",
    "Your task is to implement a part-of-speech tagger using a bigram HMM. Given an ob-\n",
    "servation sequence of n words wn\n",
    "1 , choose the most probable sequence of POS tags tn1\n",
    ".\n",
    "\n",
    "[Note: During training, for a word to be counted as unknown, the frequency of word in\n",
    "training set should not exceed a threshold (e.g. 5). You can pick a threshold based on your\n",
    "algorithm design.]\n",
    "\n",
    "\n",
    "To obtain the tag unigram counts and the tag bigram counts, you will need to separate\n",
    "out each tag from its word. For each sentence found in the training data, add a start token\n",
    "and an end token to the beginning and the end of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get dataset and append start and end tag\n",
    "def data_extractor(dataset):\n",
    "    complete_data = []\n",
    "    filename = os.getcwd() + \"\\\\\" + dataset \n",
    "    for text in os.listdir(filename):\n",
    "        filename = filename + \"\\\\\" + text\n",
    "        if \".DS_Store\" not in filename:\n",
    "            with open(filename, encoding = \"utf8\", errors = \"ignore\") as file:\n",
    "                text_in_file = file.read()\n",
    "                text_in_file = re.sub(\"\\n+\", \"\\n\", text_in_file)\n",
    "                text_in_file = re.sub(\"\\t\", \"\", text_in_file)\n",
    "                text_in_file = text_in_file.split(\"./.\")\n",
    "                list_words = []\n",
    "                for i in text_in_file:\n",
    "                    if i not in [\"\\n\", \"\", \" \"]:\n",
    "                        i = i.strip(\"\\n\")\n",
    "                        list_of_words = \"<START> \" + i + \" <END>\"\n",
    "                        word_list = re.findall(\"\\w+/\\w+|\\<START\\>|\\<END\\>\", list_of_words)\n",
    "                        list_words.extend(word_list)\n",
    "            complete_data.extend(list_words)\n",
    "        filename = os.getcwd() + \"\\\\\" + dataset \n",
    "    return complete_data\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 \n",
    "Obtain frequency counts from the collection of all the training \f",
    "les (counted together). You\n",
    "will need the following types of frequency counts: word-tag counts, tag unigram counts,\n",
    "and tag bigram counts. Lets denote these by C (wi ; ti), C (ti), and C (tiô€€€1 ; ti) respectively.\n",
    "Report these quantities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find and count all the unigrams\n",
    "text_in_file = data_extractor(dataset = \"brown-copy\")\n",
    "complete_data_list = [i.split(\"/\")[0] for i in text_in_file if \"/\" in i]\n",
    "data_unigram_count = Counter(complete_data_list)\n",
    "data_count_less_than_5 = {i: data_unigram_count[i] for i in data_unigram_count if data_unigram_count[i] < 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the less frequent words to \"UNK\" token, using replace function instead of list manipulation since it is faster\n",
    "data_text = \" \".join(text_in_file)\n",
    "for index, i in enumerate(data_count_less_than_5):\n",
    "    word = \" \" + i + \"/\"\n",
    "    data_text = data_text.replace(word , \" UNK/\")\n",
    "    if index % 1000 == 0:\n",
    "        print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count of word and tag\n",
    "data_text_list = data_text.split(\" \")\n",
    "counter_word_tag = Counter(data_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count of only tags\n",
    "tag_list = [i if i in [\"<START>\", \"<END>\"] else i.split(\"/\")[1] for i in data_text_list]\n",
    "counter_tag = Counter(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of bigrams tag\n",
    "bigram_tag_list = [\" \".join(tag_list[index:index + 2]) for index in range(len(tag_list[:-1]))]\n",
    "bigram_counter_tag = Counter(bigram_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 \n",
    "A transition probability is the probability of a tag given its previous tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## calculate the probability of transition\n",
    "prob_transition = {}\n",
    "lambda_val = 0.1\n",
    "smoothed_prob_transition = {}\n",
    "for i in bigram_counter_tag:\n",
    "    previous_tag = i.split(\" \")[0]\n",
    "    prob_transition[i] = (bigram_counter_tag[i] + lambda_val)/(counter_tag[previous_tag] + (lambda_val * len(counter_tag)))\n",
    "    smoothed_prob_transition[previous_tag] = (lambda_val)/((counter_tag[previous_tag] +lambda_val * len(counter_tag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3\n",
    "An emission probability is the probability of a given word being associated with a given tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## emission probability is the probability of a given word being associated with a given tag\n",
    "prob_emission = {}\n",
    "lambda_val = 0.1\n",
    "smoothed_prob_emission = {}\n",
    "for i in counter_word_tag:\n",
    "    if i not in  [\"<START>\", \"<END>\"]:\n",
    "        tag_of_word = i.split(\"/\")[1]\n",
    "        prob_emission[i] = (counter_word_tag[i] + lambda_val)/(counter_tag[tag_of_word] + (lambda_val * len(counter_word_tag)))\n",
    "        smoothed_prob_emission[tag_of_word] = (lambda_val)/((counter_tag[tag_of_word] +lambda_val * len(counter_word_tag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4\n",
    "\n",
    "Generate 5 random sentences using HMM. Output each sentence (with the POS tags) and\n",
    "its probability of being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate five random sentences based on emission and transition probability starting and ending with <START> and <END> yag \n",
    "list_5_rand_sentences = []\n",
    "while (len(list_5_rand_sentences) != 5):\n",
    "    prev_tag = \"<START>\"\n",
    "    tag_drawn = \" \"\n",
    "    rand_sententence = []\n",
    "    while True:\n",
    "        next_set_of_tags = [i for i in bigram_counter_tag if i.split(\" \")[0] == prev_tag and i != \"<END> <START>\"]\n",
    "        prob_next_tag = [prob_transition[i] for i in next_set_of_tags]\n",
    "        tag_drawn = random.choices(next_set_of_tags,prob_next_tag)\n",
    "        prev_tag = tag_drawn[0].split(\" \")[1]\n",
    "        if prev_tag == \"<END>\":\n",
    "            break\n",
    "        probable_words = [i for i in counter_word_tag if i not in [\"<END>\",\"<START>\"] and i.split(\"/\")[1] == prev_tag and \"UNK\" not in i]\n",
    "        word_probabilities = [prob_emission[i] for i in probable_words]\n",
    "        word_drawn = random.choices(probable_words,word_probabilities)\n",
    "        prob_of_this_word = prob_emission[word_drawn[0]] * prob_transition[tag_drawn[0]]\n",
    "        word_drawn[0] = word_drawn[0] + \" \" + str(prob_of_this_word)\n",
    "        rand_sententence.extend(word_drawn)\n",
    "    list_5_rand_sentences.append(\" \".join(rand_sententence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 1 \n",
      "\t John/np 0.0007098881965105562 William/np 0.0007291966409498122 of/in 0.018772209734886754 the/at 0.22266728573686068 grounds/nns 6.915242320338724e-05 of/in 0.07908709712941428 all/abn 0.00434994068179207 be/be 0.001543369745140279 with/in 0.002340103095761513 both/abx 0.0004348187807154439 The/at 0.007202984273656557 spite/nn 0.00015251024398632337 and/cc 0.05789187326335965 it/pps 0.007095342160281965 was/bedz 0.12110602133238504 implying/vbg 3.072770558043864e-05 \n",
      "\n",
      "**Printing the sentence properly**\n",
      "\t John William of the grounds of all be with both The spite and it was implying \n",
      "\n",
      "Sentence ID: 2 \n",
      "\t who/wps 0.0005389002024534835 get/vb 0.0021519696210883033 his/pp 0.01681628796631599 British/jj 0.0002648576310265283 to/to 0.016668149746310208 understand/vb 0.0030966175475128834 Hanover/np 3.354784438502727e-05 \n",
      "\n",
      "**Printing the sentence properly**\n",
      "\t who get his British to understand Hanover \n",
      "\n",
      "Sentence ID: 3 \n",
      "\t shaking/vbg 1.1680090182855517e-05 to/in 0.014613290979083142 an/at 0.012544165832738008 fault/nn 7.007227426398642e-05 Af/nn 0.0006134999614596458 in/in 0.04259250616386392 my/pp 0.003775854381929988 underlying/vbg 1.22435097533252e-05 the/at 0.08751171742724961 bit/nn 0.0002951913454288296 started/vbd 0.00012961388976376881 in/in 0.0264836077185481 the/at 0.22266728573686068 scientific/jj 0.0002568684921572241 existence/nn 0.0002940056162127484 \n",
      "\n",
      "**Printing the sentence properly**\n",
      "\t shaking to an fault Af in my underlying the bit started in the scientific existence \n",
      "\n",
      "Sentence ID: 4 \n",
      "\t batch/nn 1.2309915162205632e-06 with/in 0.015622572262068908 his/pp 0.021268098181163597 countries/nns 0.00048136309436081875 \n",
      "\n",
      "**Printing the sentence properly**\n",
      "\t batch with his countries \n",
      "\n",
      "Sentence ID: 5 \n",
      "\t this/dt 0.012057133216562493 will/nn 0.00029922984169964355 as/cs 0.0063515204078663755 the/at 0.1608102470389059 fair/jj 0.00018577372296810072 Supreme/jj 2.8185444401170853e-05 interview/nn 8.458440947471301e-05 with/in 0.015622572262068908 some/dti 0.0025107336782454366 symbol/nn 0.00013181722758083935 \n",
      "\n",
      "**Printing the sentence properly**\n",
      "\t this will as the fair Supreme interview with some symbol \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## print the sentences\n",
    "for index, i in enumerate(list_5_rand_sentences):\n",
    "    print (\"Sentence ID:\", index+1, \"\\n\\t\", i, \"\\n\")\n",
    "    sentence = i.split(\" \")\n",
    "    sent_list = [j.split(\"/\")[0] for j in sentence if \"/\" in j]\n",
    "    sent_str = \" \".join(sent_list)\n",
    "    print (\"**Printing the sentence properly**\\n\\t\",sent_str, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get the test dataset and format it\n",
    "def collect_testdata(filename): \n",
    "    with open(filename) as file:\n",
    "        file_content = file.read()\n",
    "    file_content = file_content.split(\"<EOS>\")\n",
    "    file_dict = {}\n",
    "    for index, content in enumerate(file_content):\n",
    "        list_content = content.split(\"\\n\")\n",
    "        list_content = [i for i in list_content if re.match(\"\\w+\", i)]\n",
    "        file_dict[index] = list_content\n",
    "    return file_dict\n",
    "\n",
    "filename = r\"C:\\Users\\mm199\\NLP\\HW1\\science_sample.txt\"\n",
    "file_dict = collect_testdata(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Implement Viterbi Algorithm\n",
    "For each word, output the tag derived using the\n",
    "Viterbi algorithm in the following format(where each line contains no more than one pair):\n",
    "<sentence ID=1>\n",
    "word, tag\n",
    "word, tag\n",
    ":::\n",
    "word, tag\n",
    "<EOS>\n",
    "<sentence ID=2>\n",
    "word, tag\n",
    "word, tag\n",
    ":::\n",
    "word, tag\n",
    "<EOS>\n",
    "\n",
    "Submit your code, a README.txt file explaining how to run your code. Please also\n",
    "include your output file with the tag predictions in the format specified above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## viterbi algorithm by just considering previous state\n",
    "def viterbi_algo(test_data, counter_tag, prob_transition, prob_emission, smoothed_prob_transition, smoothed_prob_emission):\n",
    "    len_of_test_data = len(test_data)\n",
    "    total_unique_tags = [i for i in counter_tag if i not in [\"<START>\", \"<END>\"]]\n",
    "    len_of_total_tags = len(total_unique_tags)\n",
    "    pi = np.ndarray((len_of_test_data+1, len_of_total_tags+1))\n",
    "    bp =  np.ndarray((len_of_test_data, len_of_total_tags))\n",
    "    pi[0][:] = 1.0\n",
    "    for i in range(len_of_test_data):\n",
    "        for index1, tags1 in enumerate(total_unique_tags):\n",
    "            max_tag_prob = -1\n",
    "            if i == 0:\n",
    "                all_tags = [\"<START>\"]\n",
    "            else:\n",
    "                all_tags = total_unique_tags\n",
    "            for index2, tags2 in enumerate(all_tags):\n",
    "                transition = tags2 + \" \" + tags1\n",
    "                emission = test_data[i] + \"/\" + tags1\n",
    "                if transition not in prob_transition or prob_transition[transition] == 0:\n",
    "                    prob_transition[transition] = smoothed_prob_transition[tags2]\n",
    "                else:\n",
    "                    pass\n",
    "                if emission not in prob_emission or prob_emission[emission] == 0:\n",
    "                    prob_emission[emission] = smoothed_prob_emission[tags1]\n",
    "                else:\n",
    "                    pass\n",
    "                prob = pi[i][index2] * prob_transition[transition] * prob_emission[emission]\n",
    "                if prob > max_tag_prob:\n",
    "                    max_tag_prob = prob\n",
    "                    max_index = index2\n",
    "            if max_tag_prob == 0:\n",
    "                print (index1, i, tags1, index2, tags2)\n",
    "            pi[i+1][index1] = max_tag_prob\n",
    "            bp[i][index1] = max_index\n",
    "    ## get the back pointer starting point\n",
    "    last_array = pi[(len_of_test_data)][1:]\n",
    "    for i, tags in enumerate(total_unique_tags):\n",
    "        transition = tags + \" \" + \"<END>\"\n",
    "        if transition not in prob_transition or prob_transition[transition] == 0:\n",
    "            prob_transition[transition] = smoothed_prob_transition[tags]\n",
    "        else:\n",
    "            pass\n",
    "        last_array[i] = last_array[i] * prob_transition[transition]\n",
    "    bp_start = np.argmax(last_array)\n",
    "    ## traverse from the end with the help of back pointer\n",
    "    index = len_of_test_data - 1\n",
    "    word_with_tag_result_list = [(test_data[index], total_unique_tags[bp_start])] ## initiate the tuple which contains word and its tag\n",
    "    pointer = bp_start\n",
    "    while (index != -1):\n",
    "        pointer = int (bp[index][pointer])\n",
    "        if index != 0:\n",
    "            tuple_to_be_added = (test_data[index-1], total_unique_tags[pointer])\n",
    "            word_with_tag_result_list.append(tuple_to_be_added)\n",
    "        index -= 1\n",
    "    return word_with_tag_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the result in a list of list as a tuple format\n",
    "final_tag_result = []\n",
    "for i in range(len(file_dict)):\n",
    "    test_data = file_dict[i]\n",
    "    if test_data == []:\n",
    "        break\n",
    "    word_with_tag_result_list = viterbi_algo(test_data, counter_tag, prob_transition, prob_emission, smoothed_prob_transition, smoothed_prob_emission)\n",
    "    final_tag_result.append(word_with_tag_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write the output on a file\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    for index, each_sentence in enumerate(final_tag_result):\n",
    "        start = \"<sentence ID=%s>\\n\"%(index+1)\n",
    "        file.write(start)\n",
    "        for word,tag in each_sentence[::-1]:\n",
    "            word_and_its_tag = (word + \", \" + tag + \"\\n\")\n",
    "            file.write(word_and_its_tag)\n",
    "        end = \"<EOS>\\n\\n\"\n",
    "        file.write(end)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
