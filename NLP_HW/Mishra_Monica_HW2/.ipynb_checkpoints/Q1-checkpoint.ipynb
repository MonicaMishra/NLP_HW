{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Q1\n",
    "\n",
    "Using Version 3.8.0 or higher of the Stanford parser https://nlp.stanford.edu/software/lex-parser.shtml#History, \n",
    "parse the following corpus, where each file represents one genre\n",
    "of text:\n",
    "\n",
    "https://www.dropbox.com/s/vezkx8znrheti90/Brown_tokenized_text.zip?dl=0\n",
    "\n",
    "For sentences containing 50 words or less (punctuation does not count), obtain the part-ofspeech\n",
    "tags, the context-free phrase structure grammar representation, and the typed dependency\n",
    "representation as shown in the following sample output:\n",
    "\n",
    "https://nlp.stanford.edu/software/lex-parser.shtml#Sampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = r\"C:\\\\Users\\\\mm199\\\\NLP\\\\HW2\\\\Brown_tokenized_text\"\n",
    "data_dict = {}\n",
    "filenames = []\n",
    "for name_dataset in os.listdir(filename):\n",
    "    if \".txt\" in name_dataset:\n",
    "        filename_with_dataset = filename + \"\\\\\" + name_dataset\n",
    "        with open(filename_with_dataset, encoding= \"utf8\", errors= \"ignore\") as file:\n",
    "            file_content = file.read()\n",
    "        data_dict[name_dataset] = file_content \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_dict.txt\", \"w\") as file:\n",
    "    for i in data_dict:\n",
    "        str_i = str(i) + \" : \" + str(data_dict[i]) + \"\\n\"\n",
    "        file.write(str_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'C:\\Users\\mm199\\NLP\\HW2\\stanford-corenlp-full-2018-02-27\\stanford-corenlp-full-2018-02-27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating pos tags, context-free phrase structure grammar representation and typed dependency representation\n",
    "# for all the documents\n",
    "filenames = list(full_content_dict.keys())\n",
    "for key in filenames:\n",
    "    data_dict[key+'_pos'] = []\n",
    "    data_dict[key+'_cfg'] = []\n",
    "    data_dict[key+'_dep'] = []\n",
    "    for sentences in data_dict[key].split(' . '):\n",
    "        # Number of words in a sentence (without punctuations)\n",
    "        sent_len = len(nlp.word_tokenize(re.sub(r'[^\\w\\s]','', sentences)))\n",
    "        # Filtering out sentences with 50 words or less\n",
    "        if sent_len <= 50 and sent_len!=0:\n",
    "            # POS tags\n",
    "            data_dict[key+'_pos'].append(nlp.pos_tag(sentences))\n",
    "            # Context-free phrase structure grammar representation\n",
    "            data_dict[key+'_cfg'].append(nlp.parse(sentences))\n",
    "            # Typed dependency representation\n",
    "            data_dict[key+'_dep'].append(nlp.dependency_parse(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "Using the part-of-speech tags that the parser has given you, report the number of verbs in\n",
    "each file. Also report the part-of-speech tags that you are using to identify the verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vb_prep_count(res):\n",
    "    different_vbs = {}\n",
    "    for each_res in res:\n",
    "        for i in each_res:\n",
    "            pos_tag = i[1]\n",
    "            which_prepos = i[0].lower()\n",
    "            if \"VB\" in pos_tag:\n",
    "                if pos_tag not in different_vbs:\n",
    "                    different_vbs[pos_tag] = 1\n",
    "                else:\n",
    "                    different_vbs[pos_tag] += 1\n",
    "           \n",
    "    return (different_vbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file:  government.txt\n",
      "POS VB considered:  {'VBZ': 1081, 'VBP': 823, 'VBG': 802, 'VBN': 1661, 'VB': 1706, 'VBD': 764}\n",
      "Total VB tags:  6837\n",
      "\n",
      "For file:  mystery.txt\n",
      "POS VB considered:  {'VBD': 4221, 'VBG': 893, 'VB': 2013, 'VBN': 1140, 'VBP': 732, 'VBZ': 427}\n",
      "Total VB tags:  9426\n",
      "\n",
      "For file:  news.txt\n",
      "POS VB considered:  {'VBD': 3714, 'VBZ': 1589, 'VBN': 2248, 'VB': 2535, 'VBG': 1296, 'VBP': 978}\n",
      "Total VB tags:  12360\n",
      "\n",
      "For file:  reviews.txt\n",
      "POS VB considered:  {'VBZ': 1185, 'VBD': 887, 'VBP': 492, 'VBN': 762, 'VB': 799, 'VBG': 469}\n",
      "Total VB tags:  4594\n",
      "\n",
      "For file:  romance.txt\n",
      "POS VB considered:  {'VBD': 4979, 'VB': 2325, 'VBN': 1235, 'VBG': 1180, 'VBZ': 485, 'VBP': 821}\n",
      "Total VB tags:  11025\n"
     ]
    }
   ],
   "source": [
    "total_prep = {}\n",
    "data = []\n",
    "for file in data_dict:\n",
    "    if \"pos\" in file:\n",
    "        print (\"\\nFor file: \", file[:-4])\n",
    "        different_vbs = vb_prep_count(data_dict[file])\n",
    "        print (\"POS VB considered: \", different_vbs)\n",
    "        print (\"Total VB tags: \", sum(different_vbs.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 \n",
    "Report the number of sentences parsed; do so by searching for ROOT in either the dependency\n",
    "representation or in the context-free phrase structure grammar representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file:  government.txt\n",
      "Number of sentences parsed:  2315\n",
      "\n",
      "For file:  mystery.txt\n",
      "Number of sentences parsed:  3293\n",
      "\n",
      "For file:  news.txt\n",
      "Number of sentences parsed:  3947\n",
      "\n",
      "For file:  reviews.txt\n",
      "Number of sentences parsed:  1499\n",
      "\n",
      "For file:  romance.txt\n",
      "Number of sentences parsed:  3661\n",
      "\n",
      "Total sentences parsed:  14715\n"
     ]
    }
   ],
   "source": [
    "total_prep = {}\n",
    "num_of_sentences = 0\n",
    "for file in data_dict:\n",
    "    if \"cfg\" in file:\n",
    "        print (\"\\nFor file: \", file[:-4])\n",
    "        sent = sum([i.startswith(\"(ROOT\") for i in data_dict[file]])\n",
    "        print(\"Number of sentences parsed: \", sent)\n",
    "        num_of_sentences += sent\n",
    "print(\"\\nTotal sentences parsed: \", num_of_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3\n",
    "Using the dependency representation (or the context-free phrase structure grammar repre-\n",
    "sentation) that the parser has given you, report the total number of prepositions found in\n",
    "each file. In addition, report the most common three preposition overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file:  government.txt\n",
      "Number of preposition found:  7127\n",
      "\n",
      "For file:  mystery.txt\n",
      "Number of preposition found:  5045\n",
      "\n",
      "For file:  news.txt\n",
      "Number of preposition found:  10857\n",
      "\n",
      "For file:  reviews.txt\n",
      "Number of preposition found:  4095\n",
      "\n",
      "For file:  romance.txt\n",
      "Number of preposition found:  5555\n",
      "Top preposition ' of ' : 7852\n",
      "Top preposition ' in ' : 4720\n",
      "Top preposition ' for ' : 2441\n"
     ]
    }
   ],
   "source": [
    "total_prep = {}\n",
    "num_of_sentences = 0\n",
    "for file in data_dict:\n",
    "    preposition = []\n",
    "    if \"cfg\" in file:\n",
    "        print (\"\\nFor file: \", file[:-4])\n",
    "        for i in data_dict[file]:\n",
    "            preposition.extend(re.findall(\"IN \\w+\", i))\n",
    "        prep_counter = Counter(preposition)\n",
    "        print(\"Number of preposition found: \", sum(prep_counter.values()))\n",
    "        total_prep[file] = prep_counter\n",
    "common_prep = {}\n",
    "found_across_all = {}\n",
    "for file in total_prep:\n",
    "    for key in total_prep[file]:\n",
    "        if key not in common_prep:\n",
    "            common_prep[key] = total_prep[file][key]\n",
    "            found_across_all[key] = 1\n",
    "        else:\n",
    "            found_across_all[key] += 1\n",
    "            common_prep[key] += total_prep[file][key]\n",
    "num = 0  \n",
    "common_prep = sorted(common_prep.items(),key = lambda x:x[1], reverse = True)\n",
    "for key in common_prep:\n",
    "    key_with_prep = key[0]\n",
    "    if found_across_all[key_with_prep] == len(total_prep):\n",
    "        print (\"Top preposition\",\"\\'\", key_with_prep.split(\" \")[1],\"\\' :\", key[1])\n",
    "        num += 1\n",
    "        if num == 3:\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 \n",
    "Take a look at the constituent parsing and dependency parsing results. List out two common\n",
    "errors made in each type of parsing results, and briefly discuss potential methods to reduce\n",
    "each type of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For constituent parsing \n",
    "# 1. The pos tagging seems inconsistent that results in different parsed trees. For instance a verb is tagged as noun.\n",
    "# 2. Root word is not as per expectation for some sentences\n",
    "\n",
    "\n",
    "# For Dependency parsing\n",
    "# 1. The pos tagging is not always correct which causes a change in relation between words\n",
    "# 2. The result at times is not what is expected.\n",
    "\n",
    "# Method to reduce error - \n",
    "# POS tag with the help of more data might lead to a more robust and right tags which is consistent across all that could lead to\n",
    "# less errors. Hence training with more data might reduce the errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
