{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Sentiment Analysis [40 points]\n",
    "The following training data contains 9484 sentences of movie reviews taken from Stanford\n",
    "Sentiment Analysis Tree Bank dataset https://nlp.stanford.edu/sentiment/, where\n",
    "each sentence is rated between 0 to 4 (0 - negative, 1 - somewhat negative, 2 - neutral,\n",
    "3 - somewhat positive, 4 - positive). The sentiment score is appended at the end of each\n",
    "sentence, separated by j .\n",
    "The training data is available at: https://www.dropbox.com/s/4tcyb2iefdr2jaz/train_\n",
    "data.txt?dl=0\n",
    "You can use the following tools for implementation:\n",
    "1. TensorFlow https://www.tensorflow.org/\n",
    "2. Theano http://deeplearning.net/software/theano/\n",
    "3. Scikit-learn http://scikit-learn.org/stable/index.html\n",
    "4. PyTorch http://pytorch.org/\n",
    "In addition to output and results, please submit your code along with a README.txt\n",
    "\f",
    "le explaining how to run your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get the dataset\n",
    "with open(r\"C:\\Users\\mm199\\NLP\\HW2\\q3_train_data.txt\") as training_file:\n",
    "    train_data = training_file.readlines()\n",
    "    \n",
    "with open(r\"C:\\Users\\mm199\\NLP\\HW2\\q3_test_data.txt\") as test_file:\n",
    "    test_data = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_create_dataframe_with_unigram_counts(dataset, test = False):\n",
    "    ## porter stemmer initialization and \n",
    "    pstemm = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    ## preprocess the data \n",
    "    i = 0\n",
    "    y_label = []\n",
    "    reviews_unigram = []\n",
    "    for data in dataset:\n",
    "        if test:    \n",
    "            pattern = \"[\\w\\-]+\"\n",
    "            review_statement = data.replace(\"n't\", \"not\")\n",
    "            list_of_words_each_review = [match.group() for match in re.finditer(pattern,review_statement,re.M|re.I)]\n",
    "            list_of_words_each_review_stemmed = [lemmatizer.lemmatize(i).lower() for i in list_of_words_each_review if i != \"--\" and i.lower() not in stop_words]\n",
    "            reviews_unigram.append(\" \".join(list_of_words_each_review_stemmed))\n",
    "        else:\n",
    "            split_data = data.split(\"|\")\n",
    "            review_num = split_data[1].strip(\"\\n\")\n",
    "            review_statement = split_data[0]\n",
    "            y_label.append(int(review_num))\n",
    "            pattern = \"[\\w\\-]+\"\n",
    "            review_statement = review_statement.replace(\"n't\", \"not\")\n",
    "            list_of_words_each_review = [match.group() for match in re.finditer(pattern,review_statement,re.M|re.I)]\n",
    "            list_of_words_each_review_stemmed = [lemmatizer.lemmatize(i).lower() for i in list_of_words_each_review if i != \"--\" and i.lower() not in stop_words]\n",
    "            reviews_unigram.append(\" \".join(list_of_words_each_review_stemmed))\n",
    "        \n",
    "    ## get all the unigrams from the all the words\n",
    "    unigram_words = list(set([j for i in reviews_unigram for j in i]))\n",
    "    \n",
    "    sentences = reviews_unigram\n",
    "    cv = CountVectorizer(binary=True)\n",
    "    cv.fit(sentences)\n",
    "    train_set = cv.transform(sentences)\n",
    "\n",
    "    return train_set, y_label, unigram_words, reviews_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, y_label, unigram_words_train, reviews_unigram_train = preprocess_and_create_dataframe_with_unigram_counts(train_data)\n",
    "df_test, _, unigram_words_test, reviews_unigram_test = preprocess_and_create_dataframe_with_unigram_counts(test_data, test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1\n",
    "\n",
    "Train a Multilayer Perceptron to predict sentiment score . Using unigram features as input,\n",
    "call the training and testing functions for the Multilayer Perceptron from the tool. You do\n",
    "not need to implement the learning (i.e., back-propagation) algorithm. You should have an\n",
    "input layer, two hidden layers, and an output layer; the second hidden layer should have\n",
    "10 nodes. Use 10-fold cross-validation to optimize any parameters (e.g. activation function\n",
    "or number of nodes in the first hidden layer). Use accuracy as the metric for parameter\n",
    "selection. Describe your parameter optimization process, and report the parameters for your\n",
    "best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and get prediction on multilayer perceptron\n",
    "def mlpclassifier(df_X, y, hiddenlayernodes = 10, cross_val = None):\n",
    "    y_label = [np.argmax(each_review) for each_review in y]\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(hiddenlayernodes, 10), activation='relu',alpha=0.0001, batch_size='auto', \n",
    "                    learning_rate='constant', learning_rate_init=0.001)\n",
    "    clf.fit(df_X, y)\n",
    "    if cross_val:\n",
    "        pred = cross_val_predict(clf, df_X, y, cv=10)\n",
    "    else:\n",
    "        pred = clf.predict(df_X)\n",
    "    res_pred = [np.argmax(each_review) for each_review in pred]\n",
    "    score = np.mean(np.equal(res_pred, y_label))\n",
    "    print (\"Accuracy with number_of_hidden_nodes\", (hiddenlayernodes),\" : \", score)\n",
    "    return clf, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## results prediction on test\n",
    "def pred_test_res(clf, df_X):\n",
    "    pred = clf.predict(df_X)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross validation\n",
    "def cross_validating(df_X, y_label):\n",
    "    df_param_num = [2, 5, 10, 20]\n",
    "    max_score = -1\n",
    "    for num in df_param_num:\n",
    "         _, score = mlpclassifier(df_X, y_label, hiddenlayernodes = num, cross_val = 10)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        max_num = num\n",
    "    return (max_score, max_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hot encoder form\n",
    "y_one_hot = [[0 if i != j else 1 for j in range(0, 5)] for index,i in enumerate(y_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with number_of_hidden_nodes 2  :  0.28954027836355967\n",
      "Accuracy with number_of_hidden_nodes 5  :  0.29259805989034166\n",
      "Accuracy with number_of_hidden_nodes 10  :  0.30177140447068745\n",
      "Accuracy with number_of_hidden_nodes 20  :  0.30366933783213834\n"
     ]
    }
   ],
   "source": [
    "## call the cross validation to get the best parameters\n",
    "max_score1, max_num = cross_validating(df_train_sparse, y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2\n",
    "Using the parameters for the best performing model from 3.1, re-train it on the whole training\n",
    "set, and report the accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with number_of_hidden_nodes 20  :  0.9984183888654576\n"
     ]
    }
   ],
   "source": [
    "## Run on the whole training set on the best performing model \n",
    "\n",
    "##  Considering max_num = 20 as it reduces the chances of overfitting\n",
    "\n",
    "max_num = 20\n",
    "clf1, score1 = mlpclassifier(df_train_sparse, y_one_hot, hiddenlayernodes = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3\n",
    "Use pre-trained word embeddings GoogleNews-vectors-negative300.bin.gz from Word2vec\n",
    "https://code.google.com/archive/p/word2vec/, and compute the review feature vector\n",
    "by using the average word embeddings. Do the same thing in 3.1: Use 10-fold cross-validation\n",
    "to optimize any parameters (e.g. activation function or number of nodes in the \f",
    "rst hidden\n",
    "layer). Use accuracy as the metric for parameter selection. Report the parameters for your\n",
    "best model. Then re-train the best performing model on the whole training set, and report\n",
    "the accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word embedding initialization\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\mm199\\NLP\\HW2\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get word2vec for each sentences by using average word embeddings\n",
    "def word2vec_classifier(reviews_unigram):\n",
    "    dict_word2vec = {}\n",
    "    global model\n",
    "    for index, word_list in enumerate(reviews_unigram):\n",
    "        arr = np.array([0.0 for i in range(0, 300)])\n",
    "        word_list = word_list.split(\" \")\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                arr += model.get_vector(word)\n",
    "            except KeyError:\n",
    "                continue \n",
    "        dict_word2vec[index] = arr / len(word_list)\n",
    "    df_word2vec = pd.DataFrame(dict_word2vec).T\n",
    "    return df_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get df with word embeddings\n",
    "df_word2vec_train = word2vec_classifier(reviews_unigram_train)\n",
    "df_word2vec_test = word2vec_classifier(reviews_unigram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with number_of_hidden_nodes 2  :  0.17830029523407845\n",
      "Accuracy with number_of_hidden_nodes 5  :  0.23102066638549135\n",
      "Accuracy with number_of_hidden_nodes 10  :  0.2650780261493041\n",
      "Accuracy with number_of_hidden_nodes 20  :  0.2970265710670603\n"
     ]
    }
   ],
   "source": [
    "## call the cross validation to get the best parameters for word2vec dataset\n",
    "max_score2, max_num = cross_validating(df_word2vec_train, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with number_of_hidden_nodes 20  :  0.47522142555883595\n"
     ]
    }
   ],
   "source": [
    "## run on the whole training set with the best performing model parameters with nodes in the first layer as 20\n",
    "max_num = 20\n",
    "clf2, score2 = mlpclassifier(df_word2vec_train, y_one_hot, hiddenlayernodes = max_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4\n",
    "In addition to the word embeddings, add one type of features by your own design (e.g. POS\n",
    "tags distribution) to the model in 3.3. Then re-train this model on the whole training set,\n",
    "and report the accuracy on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_distribution(reviews_unigram, unigram_words, df_word2vec):  \n",
    "    ## pos tag for each unigram\n",
    "    words_review_pos_tag = nltk.pos_tag(unigram_words)\n",
    "\n",
    "    ## total number of unique tags\n",
    "    total_tags = (set([i[1] for i in words_review_pos_tag]))\n",
    "\n",
    "    ## word and tag in a dict format\n",
    "    words_review_pos_tag_dict = {i[0]:i[1] for i in words_review_pos_tag}\n",
    "\n",
    "    ## additional feature of POS distribution\n",
    "    each_review_pos_distribution = {}\n",
    "    for index, word_list in enumerate(reviews_unigram):\n",
    "        length_of_tags = 0\n",
    "        each_review_word_tag = {i : 0 for i in total_tags}\n",
    "        word_list = word_list.split(\" \")\n",
    "        for words in word_list:\n",
    "            if words not in words_review_pos_tag_dict:\n",
    "                continue\n",
    "            tag = words_review_pos_tag_dict[words]\n",
    "            each_review_word_tag[tag] += 1\n",
    "            length_of_tags += 1\n",
    "        if length_of_tags != 0:\n",
    "            for tag in each_review_word_tag:\n",
    "                each_review_word_tag[tag] /= length_of_tags\n",
    "        each_review_pos_distribution[index] = each_review_word_tag\n",
    "    df_pos_dist = pd.DataFrame(each_review_pos_distribution).T\n",
    "    \n",
    "    ## concatenate two dataframes\n",
    "    df_word_embedding = pd.concat([df_word2vec, df_pos_dist], axis = 1)\n",
    "    \n",
    "    return df_word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word_embedding_train = pos_distribution(reviews_unigram_train, unigram_words_train, df_word2vec_train)\n",
    "df_word_embedding_test = pos_distribution(reviews_unigram_test, unigram_words_test, df_word2vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with number_of_hidden_nodes 20  :  0.49578237030788697\n"
     ]
    }
   ],
   "source": [
    "## run on the whole training set with the best performing model parameters\n",
    "max_num = 20\n",
    "clf3, score3 = mlpclassifier(df_word_embedding_train, y_one_hot, hiddenlayernodes = max_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 \n",
    "Using the best model from above (based on results from 3.2, 3.3., and 3.4), predict the senti-\n",
    "ment scores for all sentences in this test set: https://www.dropbox.com/s/jf8mr7kgt3hfv6y/\n",
    "test_data.txt?dl=0 (contains 2371 sentences, one sentence per line).\n",
    "Append your predicted sentiment score by the end of each line, separated by j, as shown in\n",
    "the training data.\n",
    "Submit this file and name it labels.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get the best classifier running with activation = \"relu\" and solver = \"adam\".\n",
    "## Also, the model seems to have overfit on the unigram count training dataset, that's why the accuracy is high\n",
    "## so, choosing the word embeddings with POS tags distribution as it is closer to the cross validation dataset and with parameter of first \n",
    "## layer (number of nodes = 20).\n",
    "\n",
    "df_X = df_word_embedding_test\n",
    "max_num = 20\n",
    "clf = MLPClassifier(hidden_layer_sizes=(max_num, 10), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', \n",
    "                    learning_rate='constant', learning_rate_init=0.001, max_iter=500, random_state=3)\n",
    "clf.fit(df_word_embedding_train, y_one_hot)\n",
    "predicted_res = clf.predict_proba(df_X)\n",
    "res = [np.argmax(i) for i in predicted_res]\n",
    "test_r = [each_review.strip(\"\\n\") + \" |\" + str(res[i]) + \"\\n\" for i,each_review in enumerate(test_data)]\n",
    "with open(\"labels.txt\", \"w+\") as file:\n",
    "    for review in test_r:\n",
    "        file.write(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
